{"cells":[{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# Training flappy by combining BC and DQL"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import logging\n\nlogging.basicConfig(filename='log/long-full-image.log',level=logging.INFO)\nlogging.info('Logging File Create')"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"import tensorflow as tf\nimport tensorflow.compat.v1 as tf\n\nimport cv2\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple, deque\nimport gym\nimport numpy as np\n\nfrom flappyBird_env import FlappyBirdEnv\nfrom flappyBird_cnn import FlappyBirdCnnEnv\n\nfrom per.per import PER\nimport logging\n\ntf.disable_v2_behavior()\n\nprint(\"Tensorflow version : \",tf.__version__)\n"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"#env = FlappyBirdEnv()\nenv = FlappyBirdCnnEnv()\n#env = make_atari('MsPacmanNoFrameskip-v4')\nobs = env.reset()\n\nprint(obs.shape)\n\nif isinstance(env, FlappyBirdCnnEnv):\n    for i in range(20):\n        _ = env.step(env.action_space.sample())\n\n    im = np.reshape(env.state, (env.state.shape[0],env.state.shape[1]))\n\n    print('Test', im)\n\n    plt.figure(figsize=(4,4))\n    plt.imshow(im, cmap='gray', vmin=-1., vmax=1.)\n    plt.axis('off')\n    plt.show()"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"def runGame(env, model):\n    done = False\n    obs = env.reset()\n    R = 0\n    while not done:\n        #obs = np.expand_dims(obs,1)\n        [action] = sess.run(model.predict, feed_dict={q_net.inputs:[obs]})\n        obs, rew, done, info = env.step(action)\n        R += rew\n    return env.score, R\n"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## **1. Deep Q Learning algorithm (DQN)**"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Experience Replay"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"class ExpReplay:\n    def __init__(self, memory_size=5000, burn_in=1000):\n        self.memory_size = memory_size\n        self.burn_in = burn_in\n\n    def burn_in_capacity(self):\n        return len(self) / self.burn_in\n        \nclass ExpReplayBuffer(ExpReplay):\n    \n    def __init__(self, memory_size=5000, burn_in=1000):\n        ExpReplay.__init__(self, memory_size, burn_in)\n        self.memory_size = memory_size\n        self.burn_in = burn_in\n        self.Buffer = namedtuple('Buffer', \n            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n        self.replay_memory = deque(maxlen=memory_size)\n\n    def sample_batch(self, batch_size=32):\n        samples = np.random.choice(len(self.replay_memory), batch_size, \n                                   replace=False)\n        batch = list(zip(*[self.replay_memory[i] for i in samples]))\n        return [np.asarray(b) for b in batch]\n\n    def append(self, state, action, reward, done, next_state):\n        self.replay_memory.append(\n            self.Buffer(state, action, reward, done, next_state))\n        \n    def __len__(self):\n        return len(self.replay_memory)\n    \nclass PrioritizedExpReplay(ExpReplay):\n    \n    def __init__(self, memory_size=5000, burn_in=1000):\n        ExpReplay.__init__(self, memory_size, burn_in)\n        self.per = PER(self.memory_size)\n        self.size = 0\n        \n    def sample_batch(self, batch_size=32):\n        mini_batch, idxs, is_weights = self.per.sample(batch_size)\n        batch = list(zip(*[i for i in mini_batch]))\n        return [np.asarray(b) for b in batch]\n        \n    def append(self, priority, state, action, reward, done, next_state):\n        self.per.add(priority, (state, action, reward, done, next_state))\n        self.size+=1\n        \n    def __len__(self):\n        return self.size"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Neural Network Architectures"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"class NeuralNetwork():\n    def __init__(self, env, name='qnet'):\n        self.name = name\n        self.input_shape = list(env.observation_space.shape)\n        self.output_shape = env.action_space.n\n        \n        self.inputs = tf.placeholder(tf.float32, shape=[None]+self.input_shape, name='{}-inputs'.format(name))\n        self.q_out = self.propagation(self.inputs)\n        self.predict = self.predict()\n    \n    def propagation(self, inputs, reuse=False):\n        pass\n\n    def predict(self):\n        pass\n\nclass Q_Network(NeuralNetwork):\n    \n    def __init__(self, env, name='qnet'):\n        NeuralNetwork.__init__(self, env, name=name)\n        \n    def propagation(self, inputs, reuse=False):\n        outputs = tf.layers.dense(inputs, 10, activation='tanh', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='{}_dense1'.format(self.name), reuse=reuse)\n        outputs = tf.layers.dropout(outputs, rate=0.2)\n        outputs = tf.layers.dense(outputs, 8, activation='tanh', kernel_initializer=tf.contrib.layers.xavier_initializer(), name='{}_dense2'.format(self.name), reuse=reuse)\n        outputs = tf.layers.dense(outputs , self.output_shape,  name='{}_dense_out'.format(self.name), reuse=reuse)#, activation='sigmoid')\n        return outputs\n        \n    def predict(self):\n        return tf.argmax(self.q_out,1)\n    \nclass Deep_Q_Network(NeuralNetwork):\n    \n    def __init__(self, input_shape, name='qnet'):\n        NeuralNetwork.__init__(self, env, name=name)\n\n        \n    def propagation(self, inputs, reuse=False):\n        conv1 = tf.layers.conv2d(inputs=inputs, filters=32, kernel_size=[8, 8], strides=(4,4), padding=\"same\", name='{}_conv2d_1'.format(self.name), activation=tf.nn.relu, reuse=reuse)\n        conv2 = tf.layers.conv2d(inputs=conv1, filters=64, kernel_size=[4, 4], strides=(2,2), padding=\"same\", name='{}_conv2d_2'.format(self.name), activation=tf.nn.relu, reuse=reuse)\n        conv3 = tf.layers.conv2d(inputs=conv2, filters=64, kernel_size=[3, 3], strides=(1,1), padding=\"same\", name='{}_conv2d_3'.format(self.name), activation=tf.nn.relu, reuse=reuse)\n\n        flat = tf.layers.flatten(conv3)\n        fc1 = tf.layers.dense(flat, 512, name='{}_dense'.format(self.name), activation=tf.nn.relu, reuse=reuse)\n        outputs = tf.layers.dense(fc1, self.output_shape, name='{}_dense_out'.format(self.name), reuse=reuse)\n        return outputs\n        \n    def predict(self):\n        return tf.argmax(self.q_out,1)"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Modèles"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"class DqnModel():\n    \n    def __init__(self, q_net, target_net, learning_rate = 0.001, batch_size=32, gamma=0.99, ddqn=True):\n        self.q_net = q_net\n        self.target_net = target_net\n        \n        self.actions = tf.placeholder(tf.int32, shape=(None,), name='action')\n        self.rewards = tf.placeholder(tf.float32, shape=(None,), name='reward')\n        self.done_flags = tf.placeholder(tf.float32, shape=(None,), name='done')\n        \n        self.states = tf.placeholder(tf.float32, shape=[None]+self.q_net.input_shape, name='state')\n        self.states_next = tf.placeholder(tf.float32, shape=[None]+self.q_net.input_shape, name='state_next')\n    \n        # Calcul for pred = Q_net(batch_states_t, batch_actions), \n        action_one_hot = tf.one_hot(self.actions, self.q_net.output_shape, 1.0, 0.0)\n        self.pred = tf.reduce_sum(self.q_net.propagation(self.states, reuse=True) * action_one_hot, reduction_indices=-1)\n\n        # Calcul for y = r_t + gamma * max_a ( Q_target(batch_states_t+1, a) )\n        if ddqn:\n            amax = tf.one_hot(tf.argmax(self.q_net.propagation(self.states_next, reuse=True), axis=-1), self.q_net.output_shape, 1.0, 0.0)\n            max_Q_target = tf.reduce_sum(self.target_net.propagation(self.states_next, reuse=True) * amax, reduction_indices=-1)\n        else:\n            max_Q_target = tf.reduce_max(self.target_net.propagation(self.states_next, reuse=True))\n        self.y = self.rewards + (1. - self.done_flags) * gamma * max_Q_target\n        \n        self.error = tf.reduce_mean(tf.abs(self.pred - self.y), name=\"error\")\n        \n        # The loss measures the mean squared error between prediction and target.\n        self.loss = tf.reduce_mean(tf.square(self.pred - tf.stop_gradient(self.y)), name=\"loss_mse_train\")\n        self.optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(self.loss, name=\"adam_optim\")"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"### **Entrainement**"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":"### Paramétrage"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"# Learning parameters\nlearning_rate = 0.00025\ngamma = .975\n\neps_debut=1.\neps = eps_debut\neps_fin= 0.1\n\nnum_max_step = 20000000\nttl_episode = 10000\n\n# Buffer parameters\nmemory_size = 10000\nlearning_starts = 10000\n\n# Other params\nbatch_size = 32\nC = 1000\nobservation_shape = list(env.observation_space.shape)\n\nprint('Obs Shape =', observation_shape)"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"with tf.Session(graph=graph) as sess:\n    if False:\n        global_step = 0\n        sess.run(tf.global_variables_initializer())\n    else:\n        global_step = 450\n        saver = tf.train.import_meta_graph('models/test-expRep-prioritized-450.meta')\n        saver.restore(sess, './models/test-expRep-prioritized-450')\n    #print(tf.trainable_variables(), '\\n\\n')\n    #print(tf.all_variables())\n    print(graph.get_all_collection_keys())\n    print(len(graph.get_collection_ref('trainable_variables')))\n    for bob in graph.get_collection_ref('trainable_variables'):\n        print(bob)\n"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"# LOG Parameters \nfreq_test = 30000\nfreq_log = 10000\nfreq_save_log = 1\nfreq_save = 200000\nmodel_name = 'long-full-image'\n\nrestore_config={'restore': True,\n                'name': 'test-full-image-2',\n                'step': 3000000\n               }\n#logging.basicConfig(filename='log/{}.log'.format(model_name), level=logging.DEBUG)"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"logging.info(\"Initialisation du modèle (Graph) ...\")\nexp_replay = ExpReplayBuffer(memory_size=memory_size , burn_in=learning_starts)\n#tf.reset_default_graph()\ngraph = tf.Graph()\nwith graph.as_default():\n    q_net = Deep_Q_Network(env, name='q-net')\n    q_target = Deep_Q_Network(env, name='target-net')\n    dqn_model = DqnModel(q_net, q_target, learning_rate=learning_rate, batch_size=batch_size,gamma=gamma)\n    saver = tf.train.Saver(max_to_keep=100)\n    t_vars = tf.trainable_variables()\n    print(t_vars)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"with tf.Session(graph=graph) as sess:\n    # Initialisation du modèle\n    if restore_config['restore']:\n        list_loss = []\n        list_rew = []\n        list_value = []\n        best_rew = 0\n        global_step = 0\n        sess.run(tf.global_variables_initializer())\n        tf.get_default_graph().finalize()\n    else:\n        file = 'log/{}.npz'.format(restore_config['name'])\n        dict_log = get_np_log(file)\n        list_loss, list_rew, list_value = list(dict_log['l_loss']),list(dict_log['l_rew']),list(dict_log['l_value'])\n        best_rew = 0\n        global_step = restore_config['step']+1\n        saver.restore(sess, './models/{}-{}'.format(restore_config['name'], restore_config['step']))\n    \n    # Création d'un buffer initial\n    logging.info(\"Initialisation du buffer d'Exp Replay ...\")\n    obs = env.reset()\n    while exp_replay.burn_in_capacity() < 1:\n        [action] = sess.run(q_net.predict, feed_dict={q_net.inputs:[obs]})\n        obs1, reward, done, _ = env.step(action)\n        \n        #error = sess.run(dqn_model.error, feed_dict={dqn_model.states:[obs],\n        #                                           dqn_model.states_next:[obs1], \n        #                                           dqn_model.actions:[action], \n        #                                           dqn_model.rewards:[reward],\n        #                                           dqn_model.done_flags:[done]})\n        exp_replay.append(obs, action, reward, done, obs1)\n        obs = obs1\n    \n    logging.info(\"Start learning ...\")\n    while global_step < num_max_step:\n        obs = env.reset()\n        local_step=0\n        while (local_step<ttl_episode):\n            # Select action\n            if np.random.rand() < eps:\n                action = env.action_space.sample()\n            else :\n                [action] = sess.run(q_net.predict, feed_dict={q_net.inputs:[obs]})\n            \n            [q_value] = sess.run(q_net.q_out, feed_dict={q_net.inputs:[obs]})\n            \n            # Execute action and observe reward\n            obs1, reward, done, _ = env.step(action)\n            \n            # Store transition\n            #error = sess.run(dqn_model.error, feed_dict={dqn_model.states:[obs], dqn_model.states_next:[obs1], \n            #                                             dqn_model.actions:[action],dqn_model.rewards:[reward],\n            #                                             dqn_model.done_flags:[done]})\n            \n            exp_replay.append(obs, action, reward, done, obs1)\n            obs = obs1\n                        \n            # Sample random mini-batch from replay buffer\n            observs, actions, rewards, done_flags, observs1 = exp_replay.sample_batch(batch_size)\n            \n            # Process update of the model\n            loss, _ = sess.run([dqn_model.loss, dqn_model.optimizer],feed_dict={dqn_model.states:observs, dqn_model.states_next:observs1, \n                                                                                dqn_model.actions:actions, dqn_model.rewards:rewards,\n                                                                                dqn_model.done_flags:done_flags})\n            \n            # \n            eps = eps_debut - (global_step*((eps_debut-eps_fin)/num_max_step))\n            if (global_step % freq_save_log == 0):\n                list_loss.append(loss)\n                list_value.append(sum(q_value))   \n            \n            # Update target_network\n            if global_step % C==0:   \n                t_vars = tf.trainable_variables()\n            \n                target_vars = [var for var in t_vars if q_target.name in var.name]\n                net_vars = [var for var in t_vars if q_net.name in var.name]\n    \n                for i, var in enumerate(net_vars):\n                    target_vars[i].load(var.eval(), sess)\n            \n            if done:\n                break\n\n            # Log training\n            if global_step % freq_log==0:\n                logging.info(\"Exploration {:.1f}%\".format(eps*100))\n                logging.info(\"Step {}/{} -- Loss = {:.6f} -- Value = {:.2f}\".format(global_step, num_max_step, loss, sum(q_value)))\n                \n            # Save current model\n            if global_step % freq_save == 0 :\n                eval_display_policy(sess, FlappyBirdCnnEnv, global_step, global_step//freq_save, model_name=\"{}-{}\".format(model_name, global_step))\n                logging.info(\"Saving model step {}\".format(global_step))\n                saver.save(sess, \"./models/{}\".format(model_name), global_step=global_step)\n            \n            # Test current model\n            if global_step % freq_test==0:\n                rew = np.array([runGame(env, q_net)[1] for _ in range(10)])\n                logging.info('Test Step {} -- Score={}, Mean={}'.format(local_step, rew , rew.mean()))\n                list_rew.append(rew.mean())\n                if rew.mean() > best_rew:\n                    best_rew = rew.mean()\n                    logging.info(\"New best model saved -> Mean rewards = {}\".format(best_rew))\n                    saver.save(sess, \"./models/best-{}\".format(model_name))\n            \n            global_step = global_step + 1 \n            local_step = local_step + 1 \n    \n    logging.info(\"Learning finished\")\n    saver.save(sess, \"./models/{}\".format(model_name), global_step=global_step)   \n    np.savez('log/{}.npz'.format(model_name), l_loss=list_loss, l_rew = list_rew, l_value=list_value)\n    \n    env.close()"},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":"print(len(list_rew))\nprint(len(list_value))\nprint(len(list_loss))\n#np.savez('log/{}.npz'.format(model_name), l_loss=list_loss, l_rew = list_rew, l_value=list_value)\n"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### **Tests and Results**"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"def display_Q(x, v_0,v_1, title, num, save_name=None):\n    fig, ax = plt.subplots()\n    ax.plot(x, v_0, '-b', label='Q(s, a=0)')\n    ax.plot(x, v_1, '-r', label='Q(s, a=1)')\n    plt.xlabel('State')\n    plt.ylabel('Q_value')\n    plt.title('{}'.format(title))\n    plt.grid()\n    leg = ax.legend();\n    fig.show()\n    if save_name is not None :\n        plt.savefig('plot_img/{}'.format(save_name))\n    \ndef display_V(x, v, title, num, save_name=None):\n    fig, ax = plt.subplots()\n    ax.plot(x, v, '-b', label='V(s)')\n    plt.xlabel('State')\n    plt.ylabel('Value')\n    plt.title('{}'.format(title))\n    plt.grid()\n    leg = ax.legend();\n    fig.show()\n    if save_name is not None :\n        plt.savefig('plot_img/{}'.format(save_name))\n        \ndef moving_average(a, n=3) :\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n    \ndef displayEnv(env, num):\n    obs = np.reshape(env.state, (env.state.shape[0],env.state.shape[1]))\n    f = plt.figure(num)\n    plt.figure(figsize=(6,6))\n    plt.imshow(obs, cmap='gray', vmin=-1., vmax=1.)\n    plt.axis('on')\n    plt.show()\n    \ndef testValue(sess, q_net, env):\n    v_0 = []\n    v_1 = []\n    dy = []\n    for i in range(env.bird.rayon, env.height-env.bird.rayon):\n        env.bird.y = i\n        [action], [value] = sess.run([q_net.predict, q_net.q_out], feed_dict={q_net.inputs:[ env.state]})\n        v_0.append(value[0])\n        v_1.append(value[1])\n        dy.append((env.bird.y + env.bird.rayon/2) - (env._get_current_plateform(env.bird).get_pos_ouv()+env._get_current_plateform(env.bird).get_size_ouv()/2))\n    return dy, v_0, v_1\n\ndef get_np_log(file_name):\n    print(\"Loading\", file_name)\n    log = np.load(file_name)\n    return {'l_loss':log['l_loss'], 'l_rew':log['l_rew'], 'l_value':log['l_value']}\n\ndef eval_display_policy(sess, cls_env, num_model, num, model_name=None):\n    import random\n    env = cls_env()\n    obs = env.reset()\n    it=0\n    while it < 90:\n        it+=1\n        obs, rew, done, info = env.step(random.randint(0,1))\n    rew = np.array([runGame(env, q_net) for _ in range(10)])\n    dy, v_0, v_1 = testValue(sess, q_net, env)\n    mean_score, mean_rew = rew[:,0].mean(), rew[:,1].mean()\n    display_V(dy, np.array(v_0)+np.array(v_1), 'Model after {} steps - Rew {:.2f}/Score {:.2f}'.format(num_model, mean_rew, mean_score), num+1, save_name=\"Value-{}\".format(model_name))        \n    display_Q(dy, v_0,v_1, 'Model after {} steps - Rew {:.2f}/Score {:.2f}'.format(num_model, mean_rew, mean_score), num+1, save_name=\"Qvalue-{}\".format(model_name))"},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":"# Affiche déroulement de l'entrainement\nfile = 'log/{}.npz'.format(model_name)\ndict_log = get_np_log(file)\nl_loss, l_rew, l_value = dict_log['l_loss'],dict_log['l_rew'],dict_log['l_value']\n\nmm_loss = moving_average(l_loss[3001000:], 1000)\nfig, axs = plt.subplots()\naxs.plot(mm_loss)\naxs.set_title('Loss')\naxs.set(xlabel='Steps', ylabel='Loss')\n\nmm_rew = moving_average(l_rew, 3)\nfig2, axs2 = plt.subplots()\naxs2.plot(mm_rew)\naxs2.set_title('Rewards')\naxs2.set(xlabel='Steps', ylabel='Reward')\n\nmm_value = moving_average(l_value[3001000:], 1000)\nfig3, axs3 = plt.subplots()\naxs3.plot(mm_value)\naxs3.set_title('Value')\naxs3.set(xlabel='Steps', ylabel='Value')"},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":"# Compare entraînements\ntest_comp = [\n    get_np_log('log/full-image3.npz'),\n    \n    get_np_log('log/test-full-image.npz'),\n #   get_np_log('log/test-full-image-2.npz'),\n]\n\ntest_comp[0].update({'label':'Test 2'})\n#test_comp[1].update({'label':'Full image retrain'})\ntest_comp[1].update({'label':'Test 1'})\n\nfig, axs = plt.subplots(1)\nfor dict_log in test_comp: \n    mm_loss = moving_average(dict_log['l_loss'], 500)\n    axs.plot(mm_loss, label='Loss {}'.format(dict_log['label']))\naxs.set_title('Loss')\naxs.set(xlabel='Steps', ylabel='Loss')\nleg = axs.legend()\nplt.savefig('plot_img/vs/1vs2-eps-loss')\n\nfig, axs = plt.subplots(1)\nfor dict_log in test_comp: \n    mm_rew = moving_average(dict_log['l_rew'], 20)\n    axs.plot(mm_rew, label='Reward {}'.format(dict_log['label']))\naxs.set_title('Reward')\naxs.set(xlabel='', ylabel='Reward')\nleg = axs.legend();\nplt.savefig('plot_img/vs/1vs2-eps-rew')\n\nfig, axs = plt.subplots(1)\nfor dict_log in test_comp: \n    mm_value = moving_average(dict_log['l_value'], 500)\n    axs.plot(mm_value, label='Value {}'.format(dict_log['label']))\naxs.set_title('Value')\naxs.set(xlabel='Steps', ylabel='Value')\nleg = axs.legend();\nplt.savefig('plot_img/vs/1vs2-eps-value')"},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":"batch = list(exp_replay.sample_batch(500))\nacts = np.asarray(batch[1])\nplt.hist(x=acts, bins=env.action_space.n, color='#0504aa', density=True,\n                            alpha=0.7, rwidth=0.9)"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"# Teste l'efficacite d'un modele\nwith tf.Session(graph=graph) as sess:\n    #saver.restore(sess, tf.train.latest_checkpoint('./models/'))\n    saver.restore(sess, './models/best-model')\n    R = np.array([runGame(env, q_net) for _ in range(10)])\n    print(R)"},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":"# Display saved models\nn_models=500000\nsave_gap = 20000\nwith tf.Session(graph=graph) as sess:\n    #saver.restore(sess, tf.train.latest_checkpoint('./models/'))\n    for i, num_model in enumerate(range(save_gap,n_models,save_gap)):\n        saver.restore(sess, './models/{}-{}'.format(model_name,num_model))\n        dy = np.linspace(-75,75, 1000)#np.arange(-75,75)\n        dy.shape = (dy.size,1)\n        value = sess.run(q_net.q_out, feed_dict={q_net.inputs:dy})\n        rew = np.array([runGame(env, q_net) for _ in range(15)])\n        mean_score, mean_rew = rew[:,0].mean(), rew[:,1].mean()\n        display_Q(dy,value[:,0],value[:,1],'Model after {} steps - Rew {}/Score {}'.format(num_model,mean_rew, mean_score),i, save_name='Qvalue-{}-{}'.format(model_name, num_model) )\n        display_V(dy,value[:,0]+value[:,1],'Model after {} steps - Rew {}/Score {}'.format(num_model,mean_rew, mean_score),i, save_name='Value-{}-{}'.format(model_name, num_model) )"},{"cell_type":"code","execution_count":129,"metadata":{},"outputs":[],"source":"with tf.Session(graph=graph) as sess:\n    #saver.restore(sess, tf.train.latest_checkpoint('./models/'))\n    saver.restore(sess, './models/model-cnn-650')\n    \n    done = False\n    obs = env.reset()\n    it=0\n    while not done:\n        it+=1\n        [action], [value] = sess.run([q_net.predict, q_net.q_out], feed_dict={q_net.inputs:[obs]})\n        #print(action, value)\n        if (it%50==0):\n            pass#displayEnv(env, it)\n            \n        obs, rew, done, info = env.step(action)\n    print(\"Score final \", env.score, it)"},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":"with tf.Session(graph=graph) as sess:\n    saver.restore(sess, './models/test-full-image-2-2550000')\n    \n    R = np.array([runGame(env, q_net) for _ in range(20)])\n    print(R[:,0].mean(), R[:,0].std())\n    print(R)"},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":"n_models=3\nsave_gap = 1\nwith tf.Session(graph=graph) as sess:\n    for num, num_model in enumerate(range(save_gap,n_models,save_gap)):\n        saver.restore(sess, './models/{}-{}'.format(model_name,num_model))\n        eval_display_policy(sess, FlappyBirdCnnEnv, num_model, num, save_name=\"{}-{}\".format(model_name, num_model))"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## 2) Behavior Cloning Algorithm"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### BC Paramétrage"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"n_epochs = 3\nbatch_size = 16\nfreq_st_log = 10\nfreq_ep_log = 1\nfreq_ep_save = 1\nn_ep_test = 10"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Modèle et entrainement"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":"class BehaviorCloningModel:\n    def __init__(self, q_net, lr=0.001):\n        self.inputs = net.inputs\n        self.net = net.q_out\n        self.learning_rate = lr\n        self.net.shape\n        self.expert_out = tf.placeholder(shape=[None, 2], dtype=tf.float32)\n        \n        self.loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.expert_out, logits=self.net), name=\"loss_cross_entrop_bc\")\n        #self.loss = tf.reduce_sum(tf.square(self.expert_out - self.net))\n        \n        trainer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n        self.optim = trainer.minimize(self.loss, name=\"adam_optim\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"restore=True\nc_rew = []\nwith graph.as_default():\n    bc = BehaviorCloning(q_net, lr=learning_rate)\n\nprint(\"Start Behavior Cloning\")\nwith tf.Session(graph=graph) as sess:\n    if restore:\n        sess.run(tf.global_variables_initializer())\n        global_step = 0\n    else:\n        print(\"No restore\")\n        #saver.restore(sess, tf.train.latest_checkpoint('./'))\n        #saver.restore(sess, 'model-10')\n        \n    num_batches = int(len(data['obs'])/batch_size)\n    print('Nombre batches : ', num_batches)\n  \n    for epoch in range(global_step, n_epochs):\n        #np.random.shuffle(data)\n        for step in range(num_batches):\n            clone_obs, clone_action = getNextBatch(data, batch_size, step)\n            loss, _ = sess.run([bc.loss, bc.optim], \n                                        feed_dict={ bc.inputs: clone_obs,\n                                                    bc.expert_out : clone_action\n                                                    })\n            if step%freq_st_log==0:\n                print('Ep {}/{} - [step {}/{}] --- Loss={}'.format(epoch+1, n_epochs, step+1, num_batches, loss))\n            if step%100==0:\n                R = np.array([runGame(env, q_net) for _ in range(n_ep_test)])\n                print('Score={}, Mean={}'.format(R, R.mean()))\n                #c_rew = np.append(c_rew, R.mean())\n        if epoch%1==0:\n            R = np.array([runGame(env, q_net)[0] for _ in range(n_ep_test)])\n            print('Score={}, Mean={}'.format(R, R[0].mean()))\n        if epoch%freq_ep_save == 0 :\n            print(\"Saving model\")\n            saver.save(sess, \"./models/model\", global_step=global_step)\n\n        global_step = global_step + 1"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Tests and Results"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"print(c_rew)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"with tf.Session(graph=graph) as sess:\n    t_vars = tf.trainable_variables()\n    print(t_vars)\n    R = np.array([runGame(env, q_net) for _ in range(n_ep_test)])\n    print('Score Mean', R.mean())"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Processing Data Expert"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"dataset_name = 'dqn-expert.npz'\ndataset_save = 'dqn-expert.npz'"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"def load(dataset_name):\n    try:\n        print('Load Dataset ', dataset_name)\n        clone = {}\n        trajs = np.load(dataset_name)\n        for key in trajs.files:\n            print(key , trajs[key].shape)\n            clone[key] = trajs[key]\n    except FileNotFoundError:\n        print('Dataset Not found -> New Dataset created')\n        clone = { 'actions': np.empty(shape=[0, 1]),\n                  'obs': np.empty(shape=[0] + list(env.observation_space.shape) ),\n                  'episode_starts': np.empty(shape=[0, 1], dtype=bool),\n                  'episode_returns': np.empty(shape=[0, 1])}\n    return clone"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"data = load(dataset_name)"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"def getNextBatch(data, batch_size, step):\n    act = np.empty(shape=[0,2])\n    for action in data['actions'][step*batch_size:step*batch_size + batch_size]:\n        val = np.zeros((2))\n        val[int(action)] = 1\n        act = np.append(act, [val], axis=0)\n    return data['obs'][step*batch_size:step*batch_size + batch_size], act"},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Create Expert Trajectories"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":"n_episode = 300\nfreq_log = 150"},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":"env = FlappyBirdEnv()\nl_score = []"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"sum_score = 0\nfor i_episode in range(n_episode):\n    obs = env.reset()\n    t_rew = 0\n    while True:\n        action = 0 if obs > 0 else 1\n        #print('Save traj [o {}, a {}]'.format(obs, action))\n        obs, reward, done, info = env.step(action)\n        t_rew += reward\n        values = {'actions':action, 'obs':[obs], 'episode_starts':done}\n        for key, val in values.items():\n            if key == 'obs':\n                data[key] = np.append(data[key], [val], axis=0)\n            else:\n                data[key] = np.append(data[key], [[val]], axis=0)\n        if done:\n            break\n    sum_score += env.score\n    data['episode_returns'] = np.append(data['episode_returns'], [[t_rew]], axis=0)\n    l_score = l_score + [env.score]\n    if i_episode%freq_log==0:\n        print(\"[Ep {}/{}] Mean Score {} episodes : {}\".format(i_episode, n_episode,freq_log,sum_score/freq_log))\n        sum_score = 0\n            \n\nenv.close()"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"np.savez(dataset_save, actions=data['actions'], obs=data['obs'],\n                     episode_returns=data['episode_returns'], episode_starts=data['episode_starts'])"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"import matplotlib.pyplot as plt\nl_score = data['episode_returns']#np.array(l_score)\n\nprint(l_score.shape)\nprint('Mean={}, Std={}'.format(l_score.mean(), l_score.std()))\n\nn, bins, patches = plt.hist(x=l_score, bins=30, color='#0504aa',\n                            alpha=0.7, rwidth=0.9)\nplt.grid(axis='y', alpha=0.75)\nplt.xlabel('Score')\nplt.ylabel('Frequency')\nplt.title('Resultats bot Flappy')\nmaxfreq = n.max()\n# Set a clean upper y-axis limit.\nplt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}